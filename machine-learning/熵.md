# 熵

最近，准备面试，复习一下以前的东西，这章会涉及熵、KL散度(相对熵)、交叉熵、决策树以及现在比较火的Wasserstein Distance 



## 熵

定义：
$$
H(x)=\sum_{x}p(x)log\ p(x) \\ or =\int -p(x)log\ p(x)dx
$$

## 相对熵

在衡量两个分布差异时，引入了相对熵：
$$
KL(p||q)=\int p(x)log\ (\frac{p(x)}{q(x)})dx
$$
上面式子简单化简就能够得到交叉熵的公式：
$$
KL(p||q)=\int p(x)log\ p(x)dx -\int p(x)log\ q(x)dx
$$
后半部分就是交叉熵公式，因为前半部分为p的信息熵，是一个定值，所以后半部分往往可以用来做Loss函数。



## 决策树

决策树主要是从信息增益的角度去建立树形结构，并且对其进行剪纸操作避免过拟合，算法有ID3, C4.5和CART等。

ID3：由增熵来决定哪个做父节点。通俗来说就是通过一个最小化错误概率。



ID5:ID3很容易造成过拟合，因为ID3约分越细。所以提出了信息增益率。
$$
I_{R}(D,A)=\frac{I(A,D)}{H_{A}(D)}
$$

$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}
$$

CART：分类回归树，通过GINI指数最小的方案分类。
$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$



## Wasserstein Distance 

实际上机器学习一直在做的事情就是，生成一个分布，使其尽可能的像输入的分布。KL散度用来做这个的时候，由于KL散度的不对成性KL(A|B)不等于KL(B|A)，这在强化学习、生成模型中导致我们无法双向衡量，所以引入了Wasserstein Distance。