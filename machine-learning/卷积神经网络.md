# 卷积神经网络

共享权值

## Relu激活函数

![img](http://upload-images.jianshu.io/upload_images/2256672-0ac9923bebd3c9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

* **速度快**，和sigmod函数需要计算指数和倒数相比，relu计算代价小。
* **减轻梯度消失**
* **稀疏性**



## 卷积层输出



## Pooling层

